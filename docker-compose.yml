# MRepo Docker Compose
#
# Quick start:
#   1. Edit this file to set your music path (replace /path/to/Music)
#   2. Run: docker compose up -d
#   3. Open http://localhost:8080
#
# With AI features (CPU):
#   docker compose --profile ai up -d
#
# With AI features (GPU):
#   docker compose --profile ai-gpu up -d
#
# AI auto-enables when the AI service is reachable. To disable AI,
# simply don't start the AI profile - no environment variable needed.

services:
  mrepo:
    image: ghcr.io/iwalton3/mrepo-web:main
    ports:
      - "8080:8080"
    volumes:
      # Database (persistent)
      - mrepo-data:/data
      # Music files (read-only, adjust path to your music library)
      - /path/to/Music:/media:ro
    environment:
      - DATABASE_PATH=/data/music.db
      - MEDIA_PATH=/media
      # AI service URL - AI auto-enables when this URL is reachable
      - AI_SERVICE_URL=${AI_SERVICE_URL:-http://mrepo-ai:5002}
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/config.js"]
      interval: 30s
      timeout: 10s
      retries: 3

  # AI Service (CPU) - Enable with: docker compose --profile ai up
  mrepo-ai:
    image: ghcr.io/iwalton3/mrepo-web-ai:main
    profiles:
      - ai
    ports:
      - "5002:5002"
    volumes:
      # Music files (same as main service, read-only)
      - /path/to/Music:/media:ro
      # Database and embeddings storage (shared with main container)
      - mrepo-data:/data
      # Model cache (persistent, prevents re-downloading)
      - ai-models:/root/.cache
    environment:
      - MEDIA_PATH=/media
      - MUSIC_DB=/data/music.db
      - EMBEDDINGS_PATH=/data/ai
      - AI_DEVICE=cpu
      - AI_BATCH_SIZE=4
    networks:
      default:
        aliases:
          - mrepo-ai
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5002/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # AI Service (GPU/CUDA) - Enable with: docker compose --profile ai-gpu up
  # Requires NVIDIA Container Toolkit
  mrepo-ai-gpu:
    image: ghcr.io/iwalton3/mrepo-web-ai-gpu:main
    profiles:
      - ai-gpu
    ports:
      - "5002:5002"
    volumes:
      - /path/to/Music:/media:ro
      - mrepo-data:/data
      - ai-models:/root/.cache
    environment:
      - MEDIA_PATH=/media
      - MUSIC_DB=/data/music.db
      - EMBEDDINGS_PATH=/data/ai
      - AI_DEVICE=cuda
      - AI_BATCH_SIZE=8
    networks:
      default:
        aliases:
          - mrepo-ai
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5002/health"]
      interval: 30s
      timeout: 10s
      retries: 3

volumes:
  mrepo-data:
  ai-models:
